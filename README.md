# Ministral-3-3B Sudoku Fine-tuning avec GRPO

Ce projet implÃ©mente le fine-tuning d'un modÃ¨le Ministral-3-3B sur une tÃ¢che de rÃ©solution de Sudoku en utilisant le **Group Relative Policy Optimization (GRPO)** avec Unsloth, puis l'export au format GGUF pour une infÃ©rence optimisÃ©e.

## ğŸ“‹ Table des matiÃ¨res

- [Vue d'ensemble](#vue-densemble)
- [PrÃ©requis](#prÃ©requis)
- [Architecture du projet](#architecture-du-projet)
- [Workflow complet](#workflow-complet)
- [Scripts principaux](#scripts-principaux)
- [DÃ©pendances et workarounds](#dÃ©pendances-et-workarounds)
- [DÃ©ploiement](#dÃ©ploiement)
- [Structure des dossiers](#structure-des-dossiers)

---

## ğŸ¯ Vue d'ensemble

Ce projet dÃ©montre:
- **Fine-tuning GRPO** d'un modÃ¨le de langage sur une tÃ¢che de raisonnement (Sudoku)
- **Conversion au format GGUF** pour llama.cpp
- **DÃ©ploiement** avec vLLM ou llama.cpp
- **Workaround** pour les bugs d'export GGUF d'Unsloth

### ModÃ¨le de base
- **ModÃ¨le**: `unsloth/Ministral-3-3B-Instruct-2512`
- **MÃ©thode**: GRPO (Group Relative Policy Optimization)
- **TÃ¢che**: GÃ©nÃ©ration de code Python pour rÃ©soudre des puzzles Sudoku

### RÃ©sultats
- ModÃ¨le fine-tunÃ© capable de gÃ©nÃ©rer des stratÃ©gies Sudoku valides
- Export GGUF rÃ©ussi (F16, Q8_0)
- DÃ©ploiement rÃ©ussi sur vLLM et llama.cpp

---

## ğŸ”§ PrÃ©requis

### Environnement requis
- **GPU**: NVIDIA avec CUDA (testÃ© sur DGX Spark GB10)
- **Python**: 3.10+
- **VRAM**: ~12GB minimum pour le fine-tuning

### Packages Python principaux
```bash
pip install unsloth torch transformers trl datasets
pip install python-dotenv huggingface-hub

# Pour l'export GGUF (installation manuelle requise)
pip install gguf>=0.17.0
pip install sentencepiece>=0.2.0
pip install protobuf>=6.0.0
```

### Configuration
CrÃ©er un fichier `.env` Ã  la racine:
```bash
HF_TOKEN=your_huggingface_token_here
```

---

## ğŸ—ï¸ Architecture du projet

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Fine-tuning GRPO (Unsloth)                              â”‚
â”‚     â†’ GÃ©nÃ¨re des adaptateurs LoRA                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. VÃ©rification LoRA (optionnel)                           â”‚
â”‚     â†’ VÃ©rifie que les tensors ne sont pas vides            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Merge LoRA + Base Model (Unsloth)                       â”‚
â”‚     â†’ ModÃ¨le 16bit complet                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Push vers HuggingFace Hub                               â”‚
â”‚     â†’ ModÃ¨le partageable et tÃ©lÃ©chargeable                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. Export GGUF (llama.cpp) â­ WORKAROUND                  â”‚
â”‚     â†’ Bypass Unsloth, utilise llama.cpp directement        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  6. Push GGUF vers HuggingFace Hub                          â”‚
â”‚     â†’ Fichiers GGUF pour llama.cpp                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Workflow complet

### Ã‰tape 1: Fine-tuning GRPO

```bash
python 1_ministral_3_rl_sudoku.py
```

**Ce script fait**:
- Charge le modÃ¨le Ministral-3-3B-Instruct
- Configure les adaptateurs LoRA (rank 32)
- EntraÃ®ne avec GRPO sur 1000 exemples de Sudoku
- Sauvegarde les adaptateurs dans `grpo_saved_lora/`

**Outputs**:
- `grpo_saved_lora/` - Adaptateurs LoRA
- `outputs/` - Checkpoints d'entraÃ®nement

**DurÃ©e estimÃ©e**: 1-3 heures selon le GPU

---

### Ã‰tape 2: VÃ©rification LoRA (optionnel)

```bash
python 2_check_lora.py
```

**Ce script fait**:
- VÃ©rifie que les tensors LoRA ne sont pas tous Ã  zÃ©ro
- Affiche le pourcentage de zÃ©ros par layer

**Output**: Validation console uniquement

---

### Ã‰tape 3: Merge des adaptateurs

```bash
python 3_merge_for_vllm_v2.py
```

**Ce script fait**:
- Charge le modÃ¨le de base
- Applique la structure PEFT (mÃªme config que le training)
- Charge les poids des adaptateurs
- Merge avec `save_pretrained_merged("merged_16bit")`
- Sauvegarde le modÃ¨le complet

**Configuration**:
```python
BASE_MODEL = "unsloth/Ministral-3-3B-Instruct-2512"
LORA_ADAPTERS_PATH = "grpo_saved_lora"
OUTPUT_DIR = "ministral_3_sudoku_vllm"
```

**Output**: `ministral_3_sudoku_vllm/` (~6GB)

**DurÃ©e estimÃ©e**: 5-10 minutes

---

### Ã‰tape 4: Push vers HuggingFace

#### Option A: Script complet (recommandÃ©)

```bash
python 4_save_to_hf_v2.py
```

**Ce script fait**:
- Charge le modÃ¨le mergÃ© depuis le disque
- Push vers HuggingFace Hub
- TÃ©lÃ©charge dans le cache local pour vLLM

**Configuration**:
```python
MERGED_MODEL_DIR = "/models/fine-tuned/ministral_3_sudoku_vllm"
HF_REPO_NAME = "applied-ai-subscr/ministral_3_sudoku_vllm"
```

#### Option B: Script simple

```bash
python 4-alt_push_manual.py
```

Version simplifiÃ©e qui upload directement le dossier sans tÃ©lÃ©chargement.

**DurÃ©e estimÃ©e**: 10-30 minutes selon la connexion

---

### Ã‰tape 5: Export GGUF â­

```bash
python 5_export_gguf_v2.py
```

**Ce script fait**:
- **Utilise directement** `llama.cpp/convert_hf_to_gguf.py`
- **Bypass Unsloth** (qui plante sur l'export GGUF)
- GÃ©nÃ¨re plusieurs quantizations:
  - **F16**: 6.4 GB (qualitÃ© originale)
  - **Q8_0**: 3.5 GB (qualitÃ© excellente)

**Configuration**:
```python
MODEL_DIR = "/workspace/model"
OUTPUT_DIR = "/workspace/model_gguf"
LLAMA_CPP_CONVERTER = "/workspace/llama.cpp/convert_hf_to_gguf.py"
```

**Output**: `model_gguf/ministral-3-3b-sudoku-{f16,q8_0}.gguf`

**DurÃ©e estimÃ©e**: 10-20 minutes

---

### Ã‰tape 6: Push GGUF vers HuggingFace

```bash
python 6_push_gguf_to_hf.py
```

**Ce script fait**:
- Upload les fichiers GGUF vers HuggingFace Hub
- GÃ©nÃ¨re un README.md pour le repo GGUF
- Affiche l'URL du modÃ¨le

**Configuration**:
```python
GGUF_DIR = "/workspace/model_gguf"
HF_REPO = "applied-ai-subscr/ministral_3_3B_sudoku_gguf"
```

**DurÃ©e estimÃ©e**: 10-30 minutes selon la connexion

---

## ğŸ“ Scripts principaux

### `1_ministral_3_rl_sudoku.py`
**RÃ´le**: Fine-tuning GRPO principal

**FonctionnalitÃ©s clÃ©s**:
- ImplÃ©mentation du jeu Sudoku (`SudokuGame` class)
- GÃ©nÃ©ration de puzzles alÃ©atoires
- Reward functions pour GRPO:
  - `function_works`: VÃ©rifie que le code est exÃ©cutable
  - `no_cheating`: PÃ©nalise les imports externes
  - `strategy_succeeds`: RÃ©compense les stratÃ©gies qui rÃ©solvent le puzzle
- Trainer GRPO avec 200 steps

**HyperparamÃ¨tres**:
```python
max_seq_length = 4096
lora_rank = 32
learning_rate = 5e-5
per_device_train_batch_size = 1
num_generations = 4
max_steps = 200
```

---

### `3_merge_for_vllm_v2.py`
**RÃ´le**: Merge LoRA + Base model

**Important**: Utilise la mÃ©thode Unsloth pour garantir la compatibilitÃ©:
1. Charge base model avec `FastVisionModel.from_pretrained()`
2. Applique structure PEFT avec `FastVisionModel.get_peft_model()`
3. Charge les poids LoRA depuis safetensors
4. Merge avec `save_pretrained_merged("merged_16bit")`

---

### `5_export_gguf_v2.py` â­ **WORKAROUND**
**RÃ´le**: Conversion GGUF (contournement du bug Unsloth)

**Contexte du workaround**:
1. Unsloth tÃ©lÃ©charge automatiquement llama.cpp
2. La fonction `model.push_to_hub_gguf()` d'Unsloth **plante**
3. Solution: Utiliser directement `convert_hf_to_gguf.py` de llama.cpp

**DÃ©pendances requises** (Ã  installer manuellement):
```bash
pip install gguf sentencepiece protobuf
```

**Pourquoi Ã§a marche**:
- Conversion directe depuis safetensors (pas de GPU nÃ©cessaire)
- Plus rapide que la mÃ©thode Unsloth
- Script officiel maintenu par llama.cpp
- Support natif de Ministral3 (ajoutÃ© en dÃ©cembre 2024)

---

## ğŸ”§ DÃ©pendances et workarounds

### ProblÃ¨me: Export GGUF via Unsloth plante

**SymptÃ´me**:
```python
# Dans ministral_3_rl_sudoku.py (lignes 628-633)
model.push_to_hub_gguf(...)  # âŒ Plante
```

**Cause**: Bug dans l'implÃ©mentation Unsloth de l'export GGUF

**Solution**: Utiliser directement llama.cpp

```python
# 5_export_gguf_v2.py
subprocess.run([
    "python", "/workspace/llama.cpp/convert_hf_to_gguf.py",
    MODEL_DIR,
    "--outfile", output_file,
    "--outtype", "q8_0",
])
```

### Packages installÃ©s manuellement

| Package | Version | Pourquoi |
|---------|---------|----------|
| `gguf` | 0.17.1 | Format GGUF (requis par convert_hf_to_gguf.py) |
| `sentencepiece` | 0.2.1 | Tokenizer Mistral/Ministral |
| `protobuf` | 6.32.0 | SÃ©rialization des donnÃ©es |

### llama.cpp tÃ©lÃ©chargÃ© par Unsloth

Quand vous exÃ©cutez `1_ministral_3_rl_sudoku.py`, Unsloth tÃ©lÃ©charge automatiquement llama.cpp dans `/workspace/llama.cpp/`. On bÃ©nÃ©ficie ensuite de ce tÃ©lÃ©chargement pour notre workaround.

---

## ğŸš€ DÃ©ploiement

### Option 1: vLLM (recommandÃ© pour production)

```bash
docker run -d \
  --name vllm_ministral_sudoku \
  --gpus all \
  --ipc=host \
  -p 8003:8000 \
  -v /workspace/ministral_3_sudoku_vllm:/model \
  nvcr.io/nvidia/vllm:25.09-py3 \
  vllm serve /model \
    --tokenizer_mode mistral \
    --config_format mistral \
    --load_format mistral \
    --gpu-memory-utilization 0.9
```

**Test**:
```bash
curl http://localhost:8003/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "/model",
    "prompt": "Create a Sudoku solving strategy...",
    "max_tokens": 512,
    "temperature": 0.7
  }'
```

---

### Option 2: llama.cpp (plus lÃ©ger)

```bash
cd /workspace/llama.cpp

# Compiler (si nÃ©cessaire)
make -j$(nproc)

# Lancer le serveur
./llama-server \
  -m /workspace/model_gguf/ministral-3-3b-sudoku-q8_0.gguf \
  -c 4096 \
  -ngl 99 \
  --port 8080
```

**Test**:
```bash
curl http://localhost:8080/completion \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Create a Sudoku solving strategy...",
    "n_predict": 512
  }'
```

---

## ğŸ“‚ Structure des dossiers

```
/workspace/
â”œâ”€â”€ 1_ministral_3_rl_sudoku.py      # Fine-tuning GRPO
â”œâ”€â”€ 2_check_lora.py                 # VÃ©rification LoRA
â”œâ”€â”€ 3_merge_for_vllm_v2.py          # Merge LoRA + Base
â”œâ”€â”€ 4_save_to_hf_v2.py              # Push vers HF (complet)
â”œâ”€â”€ 4-alt_push_manual.py            # Push vers HF (simple)
â”œâ”€â”€ 5_export_gguf_v2.py             # Export GGUF (workaround)
â”œâ”€â”€ 6_push_gguf_to_hf.py            # Upload GGUF vers HF
â”‚
â”œâ”€â”€ grpo_saved_lora/                # Adaptateurs LoRA (output step 1)
â”œâ”€â”€ ministral_3_sudoku_vllm/        # ModÃ¨le mergÃ© (output step 3)
â”œâ”€â”€ model_gguf/                     # Fichiers GGUF (output step 5)
â”‚   â”œâ”€â”€ ministral-3-3b-sudoku-f16.gguf   (6.4 GB)
â”‚   â””â”€â”€ ministral-3-3b-sudoku-q8_0.gguf  (3.5 GB)
â”‚
â”œâ”€â”€ deprecated/                     # Anciens scripts (ne plus utiliser)
â”‚   â”œâ”€â”€ gguf_format.py
â”‚   â”œâ”€â”€ save_to_hf.py
â”‚   â”œâ”€â”€ merge_for_vllm.py
â”‚   â”œâ”€â”€ merge_ministral_sudoku.py
â”‚   â””â”€â”€ export_to_gguf.py
â”‚
â”œâ”€â”€ untested/                       # Scripts non testÃ©s
â”‚   â””â”€â”€ merge_and_quantize_nvfp4.py
â”‚
â”œâ”€â”€ llama.cpp/                      # TÃ©lÃ©chargÃ© par Unsloth
â”‚   â””â”€â”€ convert_hf_to_gguf.py       # Script utilisÃ© pour workaround
â”‚
â”œâ”€â”€ .env                            # HF_TOKEN
â””â”€â”€ README.md                       # Ce fichier
```

---

## ğŸ“Š Comparaison des formats

| Format | Taille | QualitÃ© | Vitesse | Usage |
|--------|--------|---------|---------|-------|
| **16bit merged** | ~6 GB | 100% | Rapide | vLLM production |
| **GGUF F16** | 6.4 GB | 100% | Rapide | llama.cpp qualitÃ© max |
| **GGUF Q8_0** | 3.5 GB | 99% | TrÃ¨s rapide | llama.cpp recommandÃ© |
| **GGUF Q4_K_M** | ~2 GB | 95% | Rapide | llama.cpp lÃ©ger |

---

## ğŸ¤ Contributions

Ce projet utilise:
- **Unsloth**: Fine-tuning et merge ([unsloth.ai](https://unsloth.ai))
- **llama.cpp**: Conversion GGUF ([ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp))
- **TRL**: GRPO Trainer ([huggingface/trl](https://github.com/huggingface/trl))
- **Transformers**: ModÃ¨les HuggingFace

---

## ğŸ“ Notes importantes

### âš ï¸ Ne pas utiliser les fichiers dans `deprecated/`
Ces scripts sont des versions antÃ©rieures qui:
- Utilisaient l'export GGUF d'Unsloth (qui plante)
- Avaient des chemins incorrects
- Sont remplacÃ©s par les versions v2

### âœ… Workflow recommandÃ© minimal

Pour un workflow complet minimal:
```bash
# 1. Fine-tuning + training
python 1_ministral_3_rl_sudoku.py

# 2. Merge
python 3_merge_for_vllm_v2.py

# 3. Export GGUF
python 5_export_gguf_v2.py

# 4. Push vers HF (optionnel)
python 4_save_to_hf_v2.py
python 6_push_gguf_to_hf.py
```

### ğŸ› Troubleshooting

**Erreur "gguf module not found"**:
```bash
pip install gguf sentencepiece protobuf
```

**Erreur lors de l'export GGUF**:
- VÃ©rifier que `/workspace/llama.cpp/convert_hf_to_gguf.py` existe
- VÃ©rifier que le modÃ¨le mergÃ© existe dans `MODEL_DIR`

**CUDA out of memory**:
- RÃ©duire `per_device_train_batch_size` dans step 1
- Utiliser `load_in_4bit=True` pour le training

---

## ğŸ“„ License

Ce projet est basÃ© sur:
- Unsloth (Apache 2.0)
- llama.cpp (MIT)
- Ministral-3 (Apache 2.0)

---

## ğŸ‰ RÃ©sultats

ModÃ¨les disponibles sur HuggingFace:
- **ModÃ¨le mergÃ© 16bit**: `applied-ai-subscr/ministral_3_sudoku_vllm`
- **Fichiers GGUF**: `applied-ai-subscr/ministral_3_3B_sudoku_gguf`

Le modÃ¨le fine-tunÃ© est capable de gÃ©nÃ©rer des stratÃ©gies Python valides pour rÃ©soudre des puzzles Sudoku avec un taux de rÃ©ussite significativement amÃ©liorÃ© par rapport au modÃ¨le de base.
